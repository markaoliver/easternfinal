{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e35ef8",
   "metadata": {},
   "source": [
    "# DTSC-670 Foundations of Machine Learning\n",
    "## Assignment 2\n",
    "### Name: (Please Enter Your Name Before Submitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7b0ec",
   "metadata": {},
   "source": [
    "## Copyright & Academic Integrity Notice\n",
    "<span style=\"color:red\">The assignment materials provided are exclusively for students officially enrolled in the course and are intended solely for purposes associated with the course. It is strictly prohibited to distribute these materials to others. Students are expressly forbidden from uploading these documents, parts of this assignment, or solutions to any external platforms such as websites, GitHub repositories, or personal websites.</span>\n",
    "\n",
    "<span style=\"color:red\">By submitting your document to CodeGrade, you are acknowledging that you fully understand the Academic Integrity policy as outlined in both the Program Handbook and the course syllabus. All submitted work must be solely your own, and any form of collaboration is strictly prohibited. You must not seek solutions online or submit them to any external websites. At the end of the term, plagiarism tracking software will be used for this assignment. Violations of the Academic Integrity policy will result in failure on the assignment, failure in the class, and/or dismissal from the program.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10ec61",
   "metadata": {},
   "source": [
    "## Student Learning Objectives\n",
    "\n",
    "- Develop proficiency in utilizing fundamental Scikit-learn functions, gaining familiarity with their syntax and applications\n",
    "- Strengthen the understanding of the machine learning process, including exploring and preparing the data\n",
    "- Demonstrate the ability to successfully construct and employ a basic linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72009cf",
   "metadata": {},
   "source": [
    "## CodeGrade\n",
    "This assignment will be automatically graded through CodeGrade, and you will have unlimited submission attempts. To ensure successful grading, please follow these instructions carefully: Name your notebook as `assignment_2.ipynb` before submission, as CodeGrade requires this specific filename for grading purposes. Additionally, make sure there are no errors in your notebook, as CodeGrade will not be able to grade it if errors are present. Before submitting, we highly recommend restarting your kernel and running all cells again to ensure that there will be no errors when CodeGrade runs your script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711a190",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "For this assignment your objective is to utilize Scikit-learn functions, including constructing a data pipeline, to preprocess data for running a multiple linear regression model. We will create a machine learning model to predict apartment prices using the available features. \n",
    "\n",
    "This assignment will follow steps outlined in the machine learning checklist provided in the textbook's appendix. It's important to acknowledge that this assignment is meant to stay at a simple, broad overview and in a real-world scenario, extensive data exploration, diverse data refinement techniques, experimentation with multiple models, and meticulous model refinement would be conducted before reaching the final evaluation stage. \n",
    "\n",
    "### Data\n",
    "This assignment uses a dataset of advertised apartment rentals in the USA, and the original dataset can be found on [UC Irvine's Machine Learning Repository](http://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified).  However, please use the dataset provided in Brightspace as some values have been changed and features deleted. The dataset provided through Brightspace contains 10,000 instances and 10 columns.\n",
    "\n",
    "The columns in the file are as follows:\n",
    "\n",
    "    - id : unique identifier of apartment\n",
    "    - latitude : latitude where the apartment is located\n",
    "    - longitude : longitude where the apartment is located\n",
    "    - bathrooms : number of bathrooms\n",
    "    - bedrooms : number of bedrooms\n",
    "    - fee : Y/N does apartment have fee?\n",
    "    - has_photo : Y/N does apartment listing have photo?\n",
    "    - pets_allowed : what pets are allowed dogs/cats etc.\n",
    "    - square_feet : size of the apartment in square feet\n",
    "    - price : rental price of apartment (This will be our target)\n",
    "\n",
    "### Assignment Instructions\n",
    "Walk through the rest of the assignment, completing the exercises as indicated.  As you read through the markdown comments, the provided code, and create your own code, think about how each section fits into the overall machine learning process.  \n",
    "\n",
    "Once you have completed all the tasks, you are ready to submit your assignment to CodeGrade for testing. Please restart your notebook's kernel and run your code from the beginning to ensure there are no error messages. Once you have verified that the code runs without any issues, submit your .ipynb notebook file to CodeGrade for evaluation. Your notebook should be called `assignment_2.ipynb`. You have unlimited attempts for this assignment.\n",
    "\n",
    "### Table of Contents \n",
    "1. [Standard Imports](#import)\n",
    "2. [Get the Data](#data)\n",
    "3. [Explore the Data](#explore)\n",
    "4. [Prepare the Data](#prepare)\n",
    "5. [Model Selection & Evaluation](#model_selection)\n",
    "6. [Final Model Evaluation](#final_model)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f1bb9",
   "metadata": {},
   "source": [
    "## Standard Imports<a name=\"import\"></a>\n",
    "Run the code block below to import your standard imports and setup the notebook for CodeGrade grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a81e16",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Do not change this option; This allows the CodeGrade auto grading to function correctly\n",
    "pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c4d4c",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Important Reminder: While progressing through the assignment, you'll encounter the need to import different Scikit-learn functions to fulfill the tasks. We deliberately refrain from indicating precisely when these imports should be made, encouraging you to naturally develop the habit of incorporating them when needed. </span>\n",
    "\n",
    "The choice is subjective, but some experts recommend consolidating all import statements at the outset of your notebook. This approach helps others using the notebook easily identify and access the necessary imports, if any are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5c5dd",
   "metadata": {},
   "source": [
    "## Get the Data<a name=\"data\"></a>\n",
    "\n",
    "**Exercise 1:** In the code block below, import the `apartments_for_rent.csv` file and save the DataFrame as `apartments`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b40ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99127c6d",
   "metadata": {},
   "source": [
    "Let's begin by examining fundamental details about the dataset. First, we will review the columns, check the total count of non-null entries, and analyze the data types associated with each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic info about dataset\n",
    "apartments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565833f",
   "metadata": {},
   "source": [
    "The dataset comprises 10,000 instances across 10 columns. Notably, the `bathrooms`, `bedrooms`, and `pets_allowed` columns contain instances with missing data.\n",
    "\n",
    "Upon inspection of the `id` and `fee` columns, it becomes evident that these columns either offer irrelevant information for our machine learning objective or exhibit consistent values across all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"id\" column contains information that is not pertinent to our current task.\n",
    "apartments['id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"fee\" column exhibits uniform values across all instances within the dataset.\n",
    "apartments['fee'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a8df2",
   "metadata": {},
   "source": [
    "**Exercise 2:** Drop the `id` and `fee` columns from the dataset making sure that you save your changes back to the `apartments` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6d8b7",
   "metadata": {},
   "source": [
    "**Exercise 3:** Before diving deeper into the data, we should stop and create a training and a test set.\n",
    "1) Since we are trying to predict the price of an apartment, save the `price` column as a Series named `price_target`.\n",
    "2) Drop the `price` column from the `apartments` DataFrame and save the remaining columns as a DataFrame named `apartments_features`.\n",
    "3) Utilize Scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)  function, employing the `apartments_features` and `price_target` variables, to partition the data into a training set and a test set. Allocate 80% of the instances for training and 20% for testing. Set the random_state to 42 to ensure reproducibility of our results.  Assign the resulting DataFrames the following names: `X_train`, `X_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359e61f",
   "metadata": {},
   "source": [
    "## Explore the Data<a name=\"explore\"></a>\n",
    "Now that we have our training set, let's explore the data in more detail.  We'll begin by checking the descriptive statistics for our numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94138ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check descriptive statistics\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25ea97",
   "metadata": {},
   "source": [
    "By examining these statistics, you can gather insights into the data's central tendencies, spread, distribution, and potential anomalies. This aids in making informed decisions during data preprocessing, feature engineering, and model selection process.\n",
    "\n",
    "Observe that the maximum values for bathrooms, bedrooms, and square_feet appear unusually high when considering a typical apartment. In a practical situation, it would be prudent to pause and conduct a more thorough investigation. This could involve examining whether there are potential data entry inaccuracies or the presence of outliers that require removal before proceeding with further model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb7d4c",
   "metadata": {},
   "source": [
    "Let's continue exploring our data and see where the apartments are located in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create scatterplot of all rentals\n",
    "X_train.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e51328",
   "metadata": {},
   "source": [
    "To analyze our categorical features, we can inspect the distribution of values by examining their respective value counts percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec08368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check value count percentages for \"has_photo\" feature\n",
    "X_train['has_photo'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a695",
   "metadata": {},
   "source": [
    "Again, in a real-world scenario, we should consider delving deeper to determine whether a distinction exists between the values labeled as \"Thumbnail\" and those labeled as \"Yes\" in the data. Should no substantial differentiation be evident, it might be appropriate to consolidate these into a single category. Additionally, take note of the minimal percentage of apartments characterized by \"No\" values. Consequently, it could be worthwhile to evaluate whether inclusion of this column in the final model contributes significant value or if it's more appropriate to omit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check value count percentages for \"pets_allowed\" feature\n",
    "X_train['pets_allowed'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f6b9b",
   "metadata": {},
   "source": [
    "Now we can plot select numerical features and the rental prices against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3564e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot select numeric values\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# select only a few numeric attributes from the training data\n",
    "col_attributes = ['bathrooms','bedrooms','square_feet']\n",
    "\n",
    "# combine training features and target into DataFrame\n",
    "combined_df = pd.concat([X_train[col_attributes], y_train], axis=1)\n",
    "\n",
    "# plots scatterplots and histograms when variable is compared against itself\n",
    "scatter_matrix(combined_df, figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e0b1c",
   "metadata": {},
   "source": [
    "Apart from the outliers we previously addressed, it's apparent that there are potentially outliers concerning the price target as well, warranting further investigation. Analyzing these visual representations, the square feet feature seems to hold promise as a potential important predictor for price, which is logical.\n",
    "\n",
    "Finally, we can compute the standard correlation coefficient between the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select attributes\n",
    "col_attributes_2 = ['latitude','longitude','bathrooms','bedrooms','square_feet']\n",
    "\n",
    "# create new combined DataFrame that includes longitude and latitude\n",
    "combined_df_2 = pd.concat([X_train[col_attributes_2], y_train], axis=1)\n",
    "\n",
    "# check correlations\n",
    "corr_matrix = combined_df_2.corr()\n",
    "corr_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d8be78",
   "metadata": {},
   "source": [
    "While there are certainly more aspects to investigate, we'll proceed to the subsequent stage of the machine learning process â€“ starting the data preparation for the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfdffb",
   "metadata": {},
   "source": [
    "## Prepare the Data<a name=\"prepare\"></a>\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "You are now going to prepare your data for use in a machine learning model.  For our numeric data, you will fill in any missing values and then scale your data (Note: Feature scaling isn't always necessary for linear regression but can be advantageous in some cases. Gradient descent optimization benefits from scaling for quicker convergence and better performance. However, with closed-form solutions like the normal equation, scaling isn't vital as the algorithm manages varied feature scales inherently).\n",
    "\n",
    "To ensure that our steps are executed in the correct order, and to assist us in preparing our test set, you will create a pipeline for your transformations.\n",
    "\n",
    "**Exercise 4:** \n",
    "1) Utilize Scikit-Learn's [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function to generate a pipeline named `num_pipeline`.  \n",
    "2) Within this pipeline, begin by incorporating a [SimpleImputer(https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) transformation using the `median` strategy. \n",
    "3) Next, add a [StandardScalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transformation into the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92855946",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61ecc9",
   "metadata": {},
   "source": [
    "In the California Housing example, we only used the pipeline for our numeric data.  However, we can just as easily use it for our categorical data as well.  Let's setup another pipeline to impute the missing value for our categorical data (in our case, remember that the `pets_allowed` column has missing data) and then one-hot encode the data, making sure to drop the first column.\n",
    "\n",
    "**Exercise 5:** \n",
    "1) Utilize Scikit-Learn's `make_pipeline` function to generate a pipeline named `cat_pipeline`.  \n",
    "2) We are going to assume that missing values for the `pets_allowed` column means that the apartment doesn't allow pets. Within this pipeline, begin by incorporating a `SimpleImputer` transformation using the `constant` strategy along with a `fill_value` with the string \"No_Pets\".\n",
    "3) Next, add an [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) class to the pipeline, making sure that the `drop` parameter is set to \"first\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dadfaf",
   "metadata": {},
   "source": [
    "### Column Transformer\n",
    "\n",
    "Next, you will create a Column Transformer to pass your numeric data to the `num_pipeline` and your categorical features to the `cat_pipeline` you created above.\n",
    "\n",
    "**Exercise 6:**\n",
    "1) Create a list of your numerical column names (in this order): 'latitude', 'longitude', 'bathrooms', 'bedrooms', and 'square_feet'. Name this list `num_attributes`.\n",
    "2) Create a list of your categorical column names (in this order): 'has_photo' and 'pets_allowed'. Name this list `cat_attributes`.\n",
    "3) Utilize Scikit-learn's [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) function to create a transformer that:\n",
    "    - Directs the numeric data through the previously defined `num_pipeline`.\n",
    "    - Directs the categorical features through the previously defined `cat_pipeline`.\n",
    "    - Name this ColumnTransformer object `preprocessing`.\n",
    "4) Invoke the fit_transform() method on the `X_train` dataset to generate the preprocessed data. Store the resulting output in a variable named `X_train_prepared`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163df77d",
   "metadata": {},
   "source": [
    "## Model Selection & Evaluation<a name=\"model_selection\"></a>\n",
    "With our data now prepared, we can proceed to train a couple of models for predicting apartment prices. Let's begin by employing a standard linear regression model.\n",
    "\n",
    "**Exercise 7:**\n",
    "1) Utilizing Scikit-learn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class, create an instance of the class and assign the model the name `lin_reg`.\n",
    "2) Train (aka \"fit\") the `lin_reg` model using the `X_train_prepared` and `y_train` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1eb55",
   "metadata": {},
   "source": [
    "With the model trained, it's time to assess its performance using Scikit-learn's `cross_val_score` function. Run the cell below to execute a ten-fold cross-validation utilizing your `lin_reg` model, and apply the `neg_root_mean_squared_error` scoring parameter. Keep in mind that Scikit-learn employs utility functions, where higher scores are preferable, hence the necessity to negate the scores (notice the `-` sign in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6016a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a 10-fold cross validation using the `lin_reg` model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg_rmses = -cross_val_score(lin_reg, X_train_prepared, y_train,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lin_reg_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430c124",
   "metadata": {},
   "source": [
    "Observe that the average RMSE score across the cross-validation folds is rather unsatisfactory, especially when considering the median price of apartments in the training data, which is only $1275. Presently, the data seems inadequate for effectively predicting apartment prices. However, it's worthwhile to proceed and attempt another machine learning algorithm to see if there are any improvements. \n",
    "\n",
    "Let's run a Random Forest algorithm to see what happens.  At this time, don't worry about how the Random Forest algorithm works.  You will learn about this algorithm later. Note: These next two cells may take a few minutes to complete running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# instantiate a RandomForestRegressor class\n",
    "forest_reg = RandomForestRegressor(random_state=42) \n",
    "\n",
    "# fit the model\n",
    "forest_reg.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ff62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "forest_rmses = -cross_val_score(forest_reg, X_train_prepared, y_train,\n",
    "                                scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "# check cross validation scores\n",
    "pd.Series(forest_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11aa9a6",
   "metadata": {},
   "source": [
    "The Random Forest algorithm resulted in a substantial reduction of over 22% in the average RMSE score, which is a positive outcome. However, it's worth noting that the overall RMSE remains relatively high when compared to the median value of apartment prices. \n",
    "\n",
    "In a real-world scenario, it would be advisable to explore additional machine learning algorithms and consider several strategies to enhance the model's performance such as possibly increasing the quantity of data, increasing the number of features (there were a number of features removed from the original dataset for this assignment), exploring various feature engineering techniques, and fine-tuning the hyperparameters of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da30c4",
   "metadata": {},
   "source": [
    "## Final Model Evaluation<a name=\"final_model\"></a>\n",
    "We are now ready to assess our model's performance on the test set, utilizing the previously established Random Forest model. It is necessary that we apply any data transformations we performed on the training data to the testing data. It is crucial to emphasize that we should solely apply transformations to the testing data without using the \"fit_transform\" method, as we want to exclusively use the information derived from the training data for this transformation process.\n",
    "\n",
    "**Exercise 8:**\n",
    "1) Utilizing the previously established `preprocessing` ColumnTransformer, apply transformations to your `X_test` data, and label the resulting dataset as `X_test_prepared`. It is essential that you use the transform method and refrain from using the fit_transform method on your testing data.\n",
    "2) With the pre-fitted `forest_reg` model, make predictions using the `X_test_prepared` dataset and store these predictions as a variable called `final_predictions`.\n",
    "3) Utilizing Scikit-learn's [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function, calculate the root mean squared error (RMSE) by passing your y_test and final_predictions, making sure to set the `squared` parameter to `False`.  Round the RMSE score to 2 decimal places. Save this score as `final_rmse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2116ad",
   "metadata": {},
   "source": [
    "Congratulations on successfully completing the assignment! Throughout this task, you acquired and explored your dataset. By implementing pipelines, you streamlined the data preparation process, greatly simplifying the transformation of your test data. You went on to develop and assess two models for predicting apartment prices. Great job, and we hope this assignment has been a valuable learning experience!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
